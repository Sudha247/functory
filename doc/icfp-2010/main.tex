%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[preprint]{sigplanconf}

% The following \documentclass options may be useful:
%
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath,graphicx, color}

\newcommand{\Ocaml}{OCaml}
\newcommand{\unix}{\textsc{Unix}}

\begin{document}

\conferenceinfo{ICFP '10}{September 27-29, 2010, Baltimore, Maryland.} 
\copyrightyear{2010} 
\copyrightdata{[to be supplied]} 

\titlebanner{submitted to ICFP 2010}        % These are ignored unless
\preprintfooter{Experience Report: Distributed Computing for the Common Man}   % 'preprint' option specified.

\title{Experience Report: Distributed Computing for the Common Man}
%\subtitle{Subtitle Text, if any}

\authorinfo{Name1}
           {Affiliation1}
           {Email1}
\authorinfo{Name2\and Name3}
           {Affiliation2/3}
           {Email2/3}

\maketitle

\begin{abstract}
This is the text of the abstract.
\end{abstract}

\category{CR-number}{subcategory}{third-level}

\terms
term1, term2

\keywords
keyword1, keyword2

\section{Introduction}

This paper introduces a generic distributed computing library.
It was initially motivated by computing needs that exist in our own
research team. Our applications include large-scale deductive program
verification, which amounts to check the validity of a huge number of
logical formulas using a variety of automated theorem
provers~\cite{filliatre07cav}. Our computing infrastructure consists
of a few powerful multi-core machines (typically 8 to 16 cores) and
several desktop PCs (typically dual-core). However, there is no
library that helps in exploiting such a computing infrastructure in
our favorite functional programming language. Hence we designed and
implemented such a library\footnote{This library is implemented in
  \Ocaml, but the implementation should be straightforward in any
  functional programming language.}, which is the subject of this paper.

what it is and what it is not
\begin{itemize}
\item it is not a library which does parallelization
\item it is a library for reliable distributed execution of
  parallelizable computation 
\end{itemize}

inspired by Google's MapReduce~\cite{mapreduce} (itself inspired by functional
programming, ironically) ;  however, there are differences:
\begin{itemize}
\item polymorphic, higher-order API
\item more generic (cores/network)
\item does not focus on association lists
\item we don't have data locality (future work)
\end{itemize}

what is the target audience/applications: 
for instance, use if automatic provers on thousands of verification
conditions, on computing infrastructures which can be
\begin{itemize}
\item a single machine with multiple cores, possibly remote
\item several machines, small or large in number, over a network
\end{itemize}

master and worker terminology introduced here

a few technical details about marshaling should be explained

\section{API}\label{sec:API}

The main idea is to start with a generic function, with the
following interface: 
% TODO: rename this function?
\begin{ocaml}
  val compute : 
    worker:('a -> 'b) -> 
    master:('a * 'c -> 'b -> ('a * 'c) list) -> 
    ('a * 'c) list -> unit
\end{ocaml}
% TODO be even more explicit about the introduction of the word `task'
This function takes two functions, \ocaml{worker} and \ocaml{master}, and a
list of \emph{tasks} of type \ocaml{('a * 'c) list}.
The function \ocaml{worker} is applied to the first component for each task.
Whenever it completes, the task and the result are passed to the
function \ocaml{master}. This may possibly generate a new list of
tasks. The process ends when all computations are done.

Function \ocaml{worker} should be pure and is executed in parallel.
Function \ocaml{master}, on the contrary, can be impure and is never
executed in parallel with other \ocaml{master}s. Typically,
\ocaml{master} stores and processes information locally.
The next section will describe the usage of this generic interface to
perform traditional map and fold operations.

Actually, our library provides more than just a single \ocaml{compute}
function as above, but instead provides five different versions
depending on the execution context. The five possible contexts are the
following: 
\begin{enumerate}
\item \textbf{purely sequential execution:}
  this is intended for validating the results;

\item \textbf{several cores on the same machine:} 
  this implementation makes use of \unix\ processes and thus can
  provide a polymorphic \ocaml{compute} function;

\item \textbf{same executable run on master and worker machines:}
  this implementation makes use of the ability to marshal \Ocaml\
  closures and thus can still provide a polymorphic \ocaml{compute} function;

\item \textbf{master and workers are different programs, compiled with
    the same version of \Ocaml:} 
  in this case, we can no longer marshal closures but we can still
  marshal polymorphic values. As a consequence, 
  the \ocaml{compute} function is split into two
  polymorphic functions, to implement the master and workers respectively:
  \begin{ocaml}
val master : 
  ('a * 'c -> 'b -> ('a * 'c) list) -> 
  ('a * 'c) list -> unit
val worker : ('a -> 'b) -> unit -> unit
  \end{ocaml}

\item \textbf{master and workers are different programs, not even
    compiled with the same version of \Ocaml:} in this case, we can no
  longer use marshaling, so the
  \ocaml{compute} function is split into two monomorphic functions
  over strings: 
  \begin{ocaml}
val master : 
  (string * 'c -> string -> (string * 'c) list) -> 
  (string * 'c) list -> unit
val worker : (string -> string) -> unit -> unit
  \end{ocaml}
  Note that the second component of each task is still polymorphic (of
  type \ocaml{'c} here).
\end{enumerate}

Our library is organized into three modules: \of{Sequential} for the
pure sequential implementation, \of{Cores} for multiple cores on the
same machine and \of{Network} for a network of machines, respectively.
The \of{Network} module itself is organized into three sub-modules,
corresponding to contexts 3, 4 and 5 above. The different
API functions are summarized in the following table (their signatures
being given above):
\begin{center}
  \begin{tabular}{|c|c|c|c|c|}
    \hline
    \ocaml{Sequential} & \ocaml{Cores} &
    \multicolumn{3}{|c|}{\ocaml{Network}} 
    \\\cline{3-5}
    &       & \ocaml{Same} & \ocaml{Poly} & \ocaml{Mono} \\\hline
    \ocaml{compute}  & \ocaml{compute}   & \ocaml{compute}  &
    \ocaml{master}  & \ocaml{master}  \\
    & & & \ocaml{worker}  & \ocaml{worker} \\\hline
  \end{tabular}
\end{center}

\section{Derived API}\label{sec:derived}

In most cases, the easiest way to parallelize execution it to make use
of map and fold operations over lists, where processing of the list
elements are done in parallel.  Using the generic API described in the
previous section to do that is not very convenient.
Hence we provide the most commonly used list operations in our API.

The most obvious operation is the traditional map operation over
lists, that is:
\begin{ocaml}
  val map : ('a -> 'b) -> 'a list -> 'b list
\end{ocaml}
The next natural operation is a combination of map and fold
operations, that is a function like
\begin{ocaml}
 val map_fold :
   map:('a -> 'b) -> fold:('c -> 'b -> 'c) -> 
   'c -> 'a list -> 'c
\end{ocaml}
which, given two functions, an accumulator \of{a} and a list \of{l}, computes
\begin{equation}\label{eq:map-fold}
  \of{fold} ... (\of{fold} (\of{fold} ~ a ~ (\of{map} ~ x_1)) (\of{map} ~ x_2))
  ... (\of{map} ~ x_n)
\end{equation}
for some permutation $[x_1,x_2,...,x_n]$ of the list \of{l}.

We assume that the \of{map} operations are always performed in parallel.
Regarding \of{fold} operations, we distinguish two cases:
\begin{itemize}
\item either \of{fold} operations are computationally less expensive
  than \of{map} and we perform them locally;
\item or \of{fold} operations are computationally expensive and we
  perform them in parallel.
\end{itemize}
We thus provide two functions \of{map_local_fold} and \of{map_remote_fold}.

In the case of \of{map_remote_fold}, only one \of{fold} operation can
be performed at a time (possibly in parallel with \of{map}
operations), as obvious from~(\ref{eq:map-fold}). However, there are
cases where several \of{fold} operations can be performed in parallel,
as early as intermediate results of \of{fold} operations are available.
This is the case when \of{fold} is an associative operation (which
implies that type \of{'b} and \of{'c} are the same  and that the
accumulator should be a neutral element for \of{fold}). Whenever
\of{fold} is also commutative, we can perform even more \of{fold}
operations in parallel. Thus our API provides two functions
\of{map_fold_a} and \of{map_fold_ac} for these two particular cases.
The five operations of the derived API are summarized
in Figure~\ref{fig:derived}.
\begin{figure}[t]
  \begin{ocaml}
    val map : 
      f:('a -> 'b) -> 'a list -> 'b list 
    val map_local_fold : 
      map:('a -> 'b) -> fold:('c -> 'b -> 'c) -> 
      'c -> 'a list -> 'c 
    val map_remote_fold : 
      map:('a -> 'b) -> fold:('c -> 'b -> 'c) -> 
      'c -> 'a list -> 'c 
    val map_fold_ac : 
      map:('a -> 'b) -> fold:('b -> 'b -> 'b) -> 
      'b -> 'a list -> 'b 
    val map_fold_a : 
      map:('a -> 'b) -> fold:('b -> 'b -> 'b) -> 
      'b -> 'a list -> 'b
  \end{ocaml}
  \caption{Derived API.}
\label{fig:derived}
\end{figure}

These five functions are actually derived from the generic API.
The generality is achieved by implementing theses derivations as a functor.
This functor, \of{Derive}, is parameterized by the \of{compute}
function, as follows:
\begin{ocaml}
module Derive
  (X : sig
     val compute : 
       worker:('a -> 'b) -> 
       master:('a * 'c -> 'b -> ('a * 'c) list) ->
       ('a * 'c) list -> unit
   end) = struct ... end
\end{ocaml}
We now explain how each function from Figure~\ref{fig:derived} is
implemented using \of{compute} inside the functor body.

\paragraph{\of{map}.} 
Function \of{map} is easily implemented by associating an integer
index with each element (of type \of{'a}) of the input list. Thus the
list of tasks passed to \of{compute} is a list of pairs of type \of{'a * int}.
As soon as an intermediate result is available, it is passed to
\of{master}, which stores it in a local table using the index as a key
and generates no new task. Whenever \of{compute} returns, we easily
recover the output list from the values stored in the table.

\paragraph{\of{map_local_fold}.} 
Function \of{map_local_fold} is implemented using a local reference
storing the current accumulator (of type \of{'c}).
As soon as an intermediate result is available, \of{master} 
combines it with the accumulator using function \of{fold}. Whenever \of{compute}
returns, we simply return the current accumulator.

\paragraph{\of{map_remote_fold}.}
In this case, combining the accumulator with an intermediate result is
itself a task to be performed in parallel with \of{map} operations.
Thus, when an intermediate \of{map} result is available, the
accumulator may be already in use in a \of{fold} operation. Hence we
need to store the intermediate \of{map} result in a local table, to be
used as soon as the accumulator becomes available.
The \of{master} function distinguishes between \of{map} and \of{fold}
results and, accordingly, updates the table and the accumulator. For a
\of{map} result, either we combine it with the accumulator in a new
task or we simply store it in the local table. For a \of{fold} result,
either we simply store it into the accumulator or we immediately
combine it with a pending \of{map} result from the table.
Whenever \of{compute} returns, the accumulator must be available and
we simply return it.

\paragraph{\of{map_fold_ac}.}
Function \of{map_fold_ac} is easier than the previous one, since we do
not need to store more than one intermediate result. Indeed, as soon
as two intermediate results are available, we immediately combine them
using a \of{fold} operation. Thus the \of{master} function 
either stores it or combines it, depending on the availability/non
availability of the accumulator.
Whenever \of{compute} returns, the accumulator must be available and
we simply return it.

\paragraph{\of{map_fold_a}.}
Function \of{map_fold_a} is definitely the trickiest. Indeed, the
operation being only associative, we can only combine \emph{adjacent}
intermediate results. In order to perform this, we store intermediate
results in a local table according to indices of the input list. More
precisely, if we have already computed
\begin{displaymath}
  x_i \oplus x_{i+1} \oplus \dots \oplus x_j
\end{displaymath}
where $\oplus$ denotes the \of{fold} operation, then we associate this
value with indices $i,j$. As soon as a result of indices $k,i-1$ or
$j+1,k$ is available, it can be combined with the result of $i,j$.
Whenever \of{compute} returns, the table should only contain a result
corresponding to the full input list and we simply return it.


These five functions are the most natural derivations of the
\of{compute} functions. There are obviously other possible derivations
(e.g. a variant of \of{map_fold} where only \of{fold} is meaningful);
in most cases, they could be derived in a straightforward way from our
generic API.

% TODO cannot be instantiated in the monomorphic case

\section{Implementation Details}\label{sec:implem}

We now describe the implementations of the various modules introduced
in Section~\ref{sec:API}. The implementation of the \of{Sequential}
module is straightforward and does not require any explanation.

\subsection{Multiple Cores}

The \of{Cores} module implements the distributed computing library for
multiple cores on the same machine. It provides a function
\of{set_number_of_cores: int -> unit} to indicate the number of cores
to be used. The number passed to this function may actually be
different from the actual number of cores in the machine; it is rather
the number of tasks to be performed simultaneously. 
% TODO comment more on the number of cores?
Thus a program using the \of{Cores} module typically begins with 
\begin{ocaml}
open Mapreduce.Cores
let () = set_number_of_cores 8
\end{ocaml}
% TODO rename Mapreduce?

The \of{Cores} module is implemented with \unix\ processes, using the
\of{fork} and \of{wait} system calls provided by the \of{Unix} library
of \Ocaml. The idea is pretty simple. 
The \of{compute} function maintains a global table of pending tasks
and keeps track of the number of idle cores.
Whenever there a pending task and an idle core, 
a new sub-process is created using \of{Unix.fork}; 
once completed, the sub-process marshals the result 
into a local file. The main process maintains a table
mapping each sub-process ID to the input task and the local file
name. It waits for any completed sub-process using \of{Unix.wait}
and recovers the result from the local file. Then function \of{master}
is applied, which may generate new tasks.
The main loop can be depicted in the following way:
\begin{flushleft}
  \quad  \textbf{while} pending tasks $\lor$ pending sub-processes \\
  \quad  \quad \textbf{while} pending tasks $\land$ idle cores \\
  \quad  \quad \quad create new sub-process for some task \\
  \quad  \quad \textbf{wait} for any completed sub-process \\
  \quad  \quad \quad push new tasks generated by \of{master} \\
\end{flushleft}

It would be more efficient to use \unix\ pipes instead of local files
but pipes are severely restricted w.r.t. data size. To be more
generic, we make no assumption about the size of the data. Providing
an alternative implementation using pipes would be straightforward.

The scheduling of tasks to the different cores is left to the
operating system, through \of{Unix.fork}. Thus it may be the case that
two tasks are executed on the same core, even if the declared number
of cores is less or equal than the actual number of cores on the machine.
% TODO: comment on user-provided scheduling?

\subsection{Network of Machines}

The \of{Network} module implements the distributed computing library for
a network of machines. 
It provides a function \of{declare_workers: n:int -> string -> unit} 
to fill a table of worker machines. The argument \of{n} indicates the
number of cores to be used in each worker machine, analogous to 
\of{Cores} module. Thus a program using the \of{Network} module
typically begins with
\begin{ocaml}
open Mapreduce.Network
let () = declare_workers ~n:12 "machine1"
let () = declare_workers ~n:8  "machine2"
...
\end{ocaml}

The \of{Network} module is based on a traditional client/server
architecture, where each worker is a server and the master is the
client of each worker. The main execution loop is similar to the one
in the \of{Cores} module, where distant processes on remote machines
correspond to sub-processes and idle workers correspond to idle
cores. In addition to this, it also involves issues of message
transfer and fault tolerance, which are subsequently described.

\subsubsection{Protocol}\label{sec:protocol}

Messages sent from master to workers could be any of the following kinds:
\begin{description}
\item[\of{Assign(id:int, f:string, x:string)}] This message assigns a
  new task to the worker, the task being identified by the unique
  integer \of{id}. The task to be performed is given by strings \of{f}
  and \of{x}, which are interpreted depending on the context.

\item[\of{Kill(id:int)}] This message indicates the worker that task
  identified by \of{id} is to be killed.

\item[\of{Stop}] This message informs the worker about completion of
  the computation, so that it may choose to exit.

\item[\of{Ping}] This message is used to check if the worker is still
  alive, expecting a \of{Pong} message from the worker in return.
\end{description}
Messages sent by the workers could be any of the following kinds:
\begin{description}
\item[\of{Pong}] This message is an acknowledgment for a \of{Ping}
  message from the master.

\item[\of{Completed(id:int, s:string)}] This message indicates the
  completion of a task identified by \of{id}, with result \of{s}.

\item[\of{Aborted(id:int)}] This message informs the master that the
  task identified by \of{id} is aborted, either as a response to a
  \of{Kill} message or because of a worker malfunction.
\end{description}
Our implementation of the protocol works across different
architectures, so that master and workers could be run on completely
different platforms w.r.t. endianness, version of \Ocaml\ and
operating system.

\subsubsection{Fault Tolerance}\label{sec:fault}

The main issue in any distributed computing environment is ability to
handle faults, which is also a distinguishing feature of our library.
Faults are mainly of two kinds: either a worker is stopped, and
possibly later restarted; or a worker is temporarily or permanently
unreachable on the network. To provide fault tolerance, our master
implementation is keeping track of the status of each worker. 
This status is controlled by two timeout parameters $T_1$ and $T_2$
and \of{Ping} and \of{Pong} messages sent by master and workers,
respectively. There are four possible statuses for a worker: 
\begin{description}
\item[\of{not connected}:] the worker is still to be
  connected to perform any useful task;
\item[\of{alive}:] the worker has sent some message recently,
  within $T_1$ seconds;
\item[\of{pinged}:] the worker has not sent any message recently (for more
  than $T_1$ seconds) and the master hash sent it a \of{Ping} message
  within the last $T_2$ seconds;
\item[\of{unreachable}:] the worker has not yet responded to the \of{Ping}
  message (for more than $T_2$ seconds).
\end{description}
% TODO picture
Whenever we receive a message from a worker, its status changes to
\of{alive} and its timeout value is reset.

Fault tolerance is achieved by exploiting the statues of workers as
follows. First, tasks are only assigned to workers with either
\of{alive} or \of{pinged} status. Second, whenever a worker executing
a task $t$ moves to status \of{not connected} or \of{unreachable}, the
task $t$ is rescheduled, which means it is put back in the set of
pending tasks. Whenever a task is completed, any rescheduled copy of
this task is either removed from the set of pending tasks or killed if
it was already assigned to an idle worker.

\subsubsection{Three Network Implementations} % TODO: change title

As explained in Section~\ref{sec:API}, the \of{Network} module is
actually providing three different implementations, according to three
different execution scenarios (namely, \of{Same}, \of{Poly} and
\of{Mono}). There is actually a single, generic implementation, which
is then instantiated in three different ways.
The generic implementation assumes that we are only transmitting
strings over the network, as evident from \of{Assign} and
\of{Completed} messages in the protocol section~\ref{sec:protocol}.
Therefore, the signature for the generic implementation is as follows:
\begin{ocaml}
  generic_master:
    assign_job:('a -> string * string) ->
    master:('a * 'c -> string -> ('a * 'c) list)  ->
    ('a * 'c) list -> unit
\end{ocaml}
The first argument \of{assign_job} is a function indicating 
the two strings to pass inside the \of{Assign} message of the protocol.
We can now distinguish the three cases:
\begin{description}
\item[\of{Same}:] in this case, \of{assign_job} is simply returning
  the marshaled closure together with the marshaled value.
\item[\of{Poly}:] in this case, \of{assign_job} is simply
  returning the marshaled value (the first string being meaningless in
  this context).
\item[\of{Mono}:] in this case, \of{assign_job} is immediately
  returning its argument (the first string being meaningless in
  this context).
\end{description}

\section{Experiments}\label{sec:experiments}

n-queens, SMT solvers, etc.

\section{Future Work}\label{sec:future}

data locality: scheduling supplied by the user

speeding up the end of computation with duplicate tasks on idle workers
(straightforward since already done for fault tolerance)

% \appendix
% \section{Appendix Title}

% This is the text of the appendix, if you need one.

\acks

Acknowledgments, if needed.

% We recommend abbrvnat bibliography style.

\nocite{*}
\bibliographystyle{abbrvnat}
\bibliography{./biblio}

\end{document}

% LocalWords:  parallelize functor parameterized indices endianness monomorphic
