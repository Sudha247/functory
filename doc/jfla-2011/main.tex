\documentclass[twoside]{article}

\usepackage{actes,comment,amsmath,graphicx,color,url}
\usepackage[french]{babel}
\usepackage[latin1]{inputenc}

\newcommand{\Ocaml}{OCaml}
\newcommand{\functory}{\textsf{Functory}}
\newcommand{\JoCaml}{Jo{\&\!}Caml}
\newcommand{\unix}{\textsc{Unix}}
\newcommand{\oeuvre}{\oe uvre}
\newcommand{\coeur}{c\oe ur}
\newcommand{\coeurs}{c\oe urs}


\title{Functory: Une bibliothèque de calcul distribué \\ pour
  Objective Caml\footnotetext{Ce
    travail a été réalisé en partie dans le cadre du projet U3CAT
    (\emph{Unification of Critical C Code Analysis Techniques},
    ANR-08-SEGI-021).}} 

\author{Jean-Christophe Filli\^{a}tre$^{1,2,3}$ \& K. Kalyanasundaram$^3$}

\titlehead{Functory}%  a droite (page impaire)

\authorhead{Filliâtre \& Kalyanasundaram}% a gauche (page paire)

\affiliation{\begin{tabular}{rr} 
\\ 1:  CNRS / LRI UMR 8623 F-91405 Orsay
\\ 2:  Université Paris Sud F-91405 Orsay
\\ 3:  INRIA Saclay -- Île-de-France F-91893 Orsay
\\ {\tt filliatr@lri.fr},  
   {\tt kalyan.krishnamani@inria.fr}
\end{tabular}}

\begin{document}
\setcounter{page}{1}
\maketitle

\begin{abstract}
  Cet article présente \functory, une bibliothèque de calcul distribué
  pour Objective Caml. Les principales caractéristiques de cette
  bibliothèque sont (1) une interface polymorphe, (2) plusieurs
  réalisations correspondant à des contextes d'utilisation différents
  et (3) un mécanisme de tolérance aux pannes.
  Cet article détaille la conception et la réalisation de Functory et
  montre son potentiel sur de nombreux exemples.
\end{abstract}

\section{Introduction}

Cet article présente \functory, une bibliothèque de calcul distribué
pour Objective Caml. Initialement, ce travail a été motivé par des
besoins de calcul au sein de notre équipe de recherche, ProVal.
Nos applications en vérification déductive de programmes incluent
notamment la validation de très nombreuses formules logiques par des
démonstrateurs automatiques variés~\cite{filliatre07cav}.
Nos moyens de calcul consistent en quelques machines très puissantes
(typiquement 8 ou 16 \coeurs) et plusieurs ordinateurs de bureau
(typiquement 2 \coeurs). Aucune bibliothèque ne nous permettait de
tirer facilement partie de cette infrastructure de calcul dans notre
langage préféré. C'est pourquoi nous avons conçu et réalisé la
bibliothèque \functory\ qui est le sujet de cet article.
Cette bibliothèque est réalisée pour \Ocaml\ mais pourrait facilement
être adaptée pour tout autre langage fonctionnel.

\functory\ n'est pas une bibliothèque qui aide l'utilisateur à
paralléliser ses calculs. Son rôle consiste plutôt à offrir des
facilités pour distribuer, de manière sûre, des calculs déjà
identifiés comme indépendants. En particulier, \functory\ offre
plusieurs interfaces génériques pour distribuer des calculs sur différents
\coeurs\ d'une même machine ou sur un réseau de machines. Ceci
correspond exactement au contexte qui a motivé la construction de
cette bibliothèque mais aussi, très probablement, à celui de
nombreuses autres applications.
Les principales caractéristiques de \functory\ sont les suivantes :
\begin{itemize}
\item \emph{généricité} : les
  traits fonctionnels que sont l'ordre
  supérieur et le polymorphisme sont exploités pour fournir un maximum
  de généricité ;
\item \emph{simplicité} : on passe d'un calcul séquentiel à un
  calcul multi-\coeurs puis à un calcul en réseau en ne modifiant
  que quelques lignes dans le code ;
\item \emph{distribution et tolérance aux pannes} : l'intégralité de
  la gestion de la distribution et de la tolérance aux pannes est
  prise en charge par la bibliothèque.
\end{itemize}
Bien que \functory\ a été écrite avec un soucis de généralité, elle ne
vise pas les grandes grappes de serveurs ou les fermes de calcul. 
Elle s'adresse plutôt aux équipes de recherche qui souhaitent
exploiter rapidement des capacités de calcul existantes allant d'un
simple ordinateur de bureau à un réseau de machines.
Le reste de cette introduction décrit notre approche du calcul
distribué dans le contexte d'un langage fonctionnel.

\paragraph{Calcul distribué.}
Initialement inspirée par la bibliothèque MapReduce de
Google~\cite{mapreduce}, notre approche lui emprunte beaucoup de
vocabulaire.
La bibliothèque \functory\ est centrée autour de la notion de
\emph{tâche}. Les tâches représentent les calculs atomiques pouvant
être réalisés de manière indépendante.
Elles sont traitées par un \emph{patron} et des \emph{ouvriers}
(respectivement \emph{master} and \emph{workers} en anglais).
Les ouvriers représentent les capacités de calcul qui effectuent les
tâches. Ils sont
matérialisés par un ou plusieurs programmes, s'exécutant en parallèle
sur une ou plusieurs machines.
Le rôle du patron consiste à distribuer les tâches auprès des ouvriers
et à récolter les résultats. Il est matérialisé par un unique
programme s'exécutant de manière séquentielle.

Une partie importante du travail de \functory\ consiste en la
transmission des tâches et de leurs résultats. 
Ceci implique leur sérialisation (en anglais
\emph{marshalling}) à travers le
réseau, sur un parc potentiellement hétérogène en termes
d'architectures et de systèmes d'exploitation.
La taille du mot et l'\emph{endianness} peuvent notamment
varier\footnote{Les auteurs se refusent à utiliser le mot \og boutisme
  \fg\ pour désigner l'\emph{endianness}.}.
Un autre aspect essentiel du calcul distribué, et de \functory\ en
particulier, est la \emph{tolérance aux pannes}. Les ouvriers peuvent
ainsi être arrêtés, relancés, temporairement stoppés ou inatteignables
à cause de problèmes liés au réseau, sans jamais compromettre le
résultat final du calcul.

\paragraph{Une approche fonctionnelle.}
Nous avons essayé de tirer partie des spécificités de la programmation
fonctionnelle pour proposer une interface la plus générique possible.
L'une des idées principales de \functory\ est que chaque ouvrier est
une fonction potentiellement polymorphe
\begin{ocaml}
  worker: 'a -> 'b
\end{ocaml}
où \of{'a} dénote le type des tâches et \of{'b} le type des résultats.
Le patron est pour sa part une fonction à laquelle on passe la liste
des tâches initiales, ainsi qu'une fonction pour traiter les résultats :
\begin{ocaml}
  master: ('a -> 'b -> 'a list) -> 'a list -> unit
\end{ocaml}
Cette fonction passée en argument à \of{master} est appliquée dès
qu'un nouveau résultat est disponible. Elle peut produire de nouvelles
tâches (d'où son type de retour \of{'a list}), qui s'ajoutent alors à
la liste des tâches à effectuer.
Le processus complet s'achève lorsque toutes les tâches ont été
effectuées. 

Notre bibliothèque tire partie des capacités de sérialisation
d'\Ocaml\ autant que possible. Ainsi lorsque le patron et les ouvriers
sont matérialisés par le même exécutable, fonctions et valeurs
polymorphes peuvent être sérialisées, ce qui permet de réaliser
facilement le schéma ci-dessus. Il n'est cependant pas toujours
possible d'utiliser le même programme pour le patron et les
ouvriers. Dans ce cas, on peut tout de même continuer à sérialiser des
valeurs polymorphes lorsque la version d'\Ocaml\ utilisée est la même
pour tous. Sinon, on ne peut plus que transmettre des chaînes de
caractères entre les différents acteurs. La bibliothèque \functory\
s'adapte à toutes ces situations en proposant plusieurs interfaces.
 
\medskip

Cet article s'organise ainsi.
La section~\ref{sec:API} présente l'interface de la bibliothèque.
Son utilisation est alors illustrée sur des exemples dans la
section~\ref{sec:studies}.
La section~\ref{sec:implem} donne des détails techniques concernant la
réalisation de \functory. Enfin la section~\ref{sec:experiments}
montre le potentiel de cette bibliothèque sur des tests expérimentaux.
%
\functory\ est librement distribuée à l'adresse
\url{http://functory.lri.fr/}. Un rapport plus détaillé que le présent
article, en anglais, est également disponible sur ce site.

\section{Interface}\label{sec:API}
Cette section décrit l'interface de la bibliothèque \functory.
La fonctionnalité primitive est une fonction \of{compute} réalisant
l'idée principale évoquée dans l'introduction.
\begin{ocaml}
  val compute : 
    worker:('a -> 'b) -> 
    master:('a * 'c -> 'b -> ('a * 'c) list) -> 
    ('a * 'c) list -> unit
\end{ocaml}
Les tâches sont des paires, de type \of{'a * 'c}, où la première
composante sera transmise à l'ouvrier et la seconde conservée
par le patron. La fonction \of{worker} doit être observationnellement
pure et sera exécutée en parallèle par tous les ouvriers. La fonction
\of{master}, au contraire, peut être impure et sera exécutée
uniquement au sein d'un unique processus séquentiel.
Cette fonction \of{master} accumule typiquement les résultats renvoyés
par les ouvriers dans une structure locale. Elle peut en outre
produire de nouvelles tâches, 
sous la forme d'une liste de type \of{('a * 'c) list}, 
qui sont alors ajoutées aux tâches restant à effectuer.
Le troisième argument de \of{compute} est la liste des tâches
initiales, qui déclenchent le calcul.
La fonction \of{compute} rend la main lorsque toutes les tâches ont
été effectuées. Il n'y a pas de résultat renvoyé, mais uniquement des
effets de bord de la fonction \of{master}.
Nous montrerons à la fin de cette section comment en déduire des
fonctions d'ordre supérieur usuelles telles que \of{map} ou \of{fold}.

En réalité, la bibliothèque \functory\ fournit \emph{cinq}
mises en {\oe}uvre différentes de la fonction \of{compute}, correspondant à
cinq contextes d'utilisation différents.
Les deux premiers sont les plus simples.
\begin{enumerate}
\item \textbf{Exécution purement séquentielle :}
  Elle permet d'obtenir un code de référence, pour mesurer des
  performances ou mettre au point son programme.

\item \textbf{Plusieurs \coeurs\ sur une même machine :} 
  Il s'agit là de distribuer le calcul sur une unique machine,
  uniquement en créant des processus fils.
\end{enumerate}
Les trois contextes suivants correspondent à une distribution du
calcul sur un réseau de machines.
\begin{enumerate}
\setcounter{enumi}{2}
\item \textbf{Patron et ouvriers sont matérialisés par un même exécutable :}
  Cette mise en {\oe}uvre exploite la capacité d'\Ocaml\ à sérialiser
  fonctions et valeurs polymorphes de manière portable.
  Selon que le programme est exécuté comme le patron ou comme un
  ouvrier, les arguments pertinents de la fonction \of{compute} sont utilisés.

\item \textbf{Patron et ouvriers sont matérialisés par différents
    programmes, compilés avec la même version d'\Ocaml\ :} 
  Il n'est plus possible de sérialiser des fonctions mais on peut
  encore sérialiser des valeurs polymorphes.
  En conséquence, la fonction \of{compute} est présentée sous la forme
  de deux fonctions, servant respectivement à réaliser le patron et
  les ouvriers :
\vspace{-0.5em}
\begin{ocaml}
val Worker.compute : ('a -> 'b) -> unit
val Master.compute : ('a * 'c -> 'b -> ('a * 'c) list) -> ('a * 'c) list -> unit
\end{ocaml}

\item \textbf{Patron et ouvriers sont matérialisés par différents
    programmes, qui ne sont même pas compilés avec la même version
    d'\Ocaml\ :} Il n'est plus possible d'utiliser la sérialisation
  d'\Ocaml\ et la fonction \of{compute} est présentée sous la forme
  de deux fonctions ne manipulant plus que des chaînes de caractères :%
\vspace{-0.5em}
\begin{ocaml}
val Worker.compute : (string -> string) -> unit
val Master.compute : (string * 'c -> string -> (string * 'c) list) ->
                    (string * 'c) list -> unit
\end{ocaml}
\end{enumerate}
La bibliothèque \functory\ est donc organisée en trois modules :
\of{Sequential} pour le calcul purement séquentiel ; \of{Cores} pour
le calcul distribué sur plusieurs \coeurs\ d'une même machine ; et
enfin \of{Network} pour le calcul en réseau. Ce dernier module
comporte trois sous-modules, appelés respectivement \of{Same},
\of{Poly} and \of{Mono}, correspondant aux situations 3, 4 et 5 ci-dessus.

%%% derived API %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Fonctions dérivées.}\label{sec:derived}
Dans de nombreuses situations, la mise en \oeuvre\ la plus simple du
parallélisme consiste à appliquer une opération sur une liste, le
traitement de chaque élément pouvant être réaliser en parallèle.
\functory\ fournit plusieurs fonctions d'ordre supérieur
offrant du calcul parallèle sur des listes, toutes dérivées de la fonction
\of{compute}. En particulier, elles sont disponibles dans les cinq
modules décrits ci-dessus.

L'opération la plus naturelle est celle consistant à appliquer une
fonction à tous les éléments d'une liste, c'est-à-dire
\begin{ocaml}
  val map : ('a -> 'b) -> 'a list -> 'b list
\end{ocaml}
Plus subtilement, on peut combiner une fonction \of{map:'a -> 'b}
avec une fonction \of{fold:'c -> 'b -> 'c} pour calculer, à partir d'une liste
$l$  et d'un accumulateur initial $a$, la valeur finale
\begin{equation}\label{eq:map-fold}
  \of{fold} ~ ... ~ (\of{fold} ~ (\of{fold} ~ a ~ (\of{map} ~ x_1)) ~
  (\of{map} ~ x_2)) ~ ... ~ (\of{map} ~ x_n)
\end{equation}
pour une certaine permutation non spécifiée $[x_1,x_2,...,x_n]$ 
de la liste \of{l}.
On peut alors distinguer deux cas, selon que l'opération \of{fold} est
beaucoup moins coûteuse que l'opération \of{map}, et peut être
effectuée localement par le patron, ou qu'au contraire elle peut être
coûteuse et a donc intérêt à être effectuée en parallèle des
opérations \of{map}. La bibliothèque fournit donc deux fonctions,
correspondant à ces deux cas de figure :
\begin{ocaml}
  val map_{local,remote}_fold : map:('a -> 'b) -> fold:('c -> 'b -> 'c) -> 'c -> 'a list -> 'c   
\end{ocaml}

Dans le cas de \of{map_remote_fold}, une seule opération \of{fold}
peut être effectuée à la fois (possiblement en parallèle d'opérations
\of{map}), comme le montre l'équation~(\ref{eq:map-fold}).
Il existe cependant des situations dans lesquelles plusieurs
opérations \of{fold} peuvent être effectuées en parallèle, dès que des
résultats intermédiaires de \of{map} sont disponibles. C'est le cas
notamment lorsque l'opération \of{fold} est associative (ce qui
implique que les types \of{'b} et \of{'c} sont égaux).
Lorsque l'opération \of{fold} est de plus commutative, on peut
effectuer encore plus d'opérations \of{fold} en parallèle.
Notre interface fournit donc deux autres fonctions pour ces cas
particuliers :
\begin{ocaml}
  val map_fold_ac : map:('a -> 'b) -> fold:('b -> 'b -> 'b) -> 'b -> 'a list -> 'b 
  val map_fold_a  : map:('a -> 'b) -> fold:('b -> 'b -> 'b) -> 'b -> 'a list -> 'b
\end{ocaml}

\hrulefill ICI\hrulefill

\medskip
These five functions are the most natural derivations of the
\of{compute} functions. There are obviously other possible derivations
(e.g. a variant of \of{map_fold} where only \of{fold} is meaningful);
in most cases, they could be derived in a straightforward way from our
generic API.

% TODO : insister sur la simplicité de Same, pour les petits programmes
% notamment 

% TODO cannot be instantiated in the monomorphic case

\section{Illustrative Case Studies}\label{sec:studies}

In this section, we show how to use \functory\ on several case studies.
We focus here on the use of the API. Experimental results of these
case studies are given in Section~\ref{sec:experiments}.
The source code for all these case studies is contained in the
distribution, in sub-directory \texttt{tests}.

\subsection{Matrix Multiplication}\label{sec:matrix}

As a first example, let us consider matrix multiplication.
We assume two matrices \of{a} and \of{b}, respectively of size
$\of{n}\times\of{p}$ and $\of{p}\times\of{m}$, given as input, as well as
a matrix \of{c} of size $\of{n}\times\of{m}$ to store the result.
Assuming \of{a} being a row-matrix and \of{b} a column-matrix,
the standard matrix multiplication would be as follows:
\begin{ocaml}
  for i = 0 to n-1 do
    for j = 0 to p-1 do
      for k = 0 to m-1 do
	c.(i).(j) <- c.(i).(j) + a.(i).(k) * b.(j).(k)
      done
    done
  done  
\end{ocaml}
and runs in $O(\of{n}\times\of{p}\times\of{m})$, 
assuming addition and multiplication over coefficients to be 
denoted $+$ and $\times$ respectively.
An easy way to distribute this computation is obviously to turn each
computation of the inner loop over \of{k} into a task.
To do so, we first prepare the list of tasks:
\begin{ocaml}
  let tasks = 
    let l = ref [] in
    for i = 0 to n-1 do for j = 0 to p-1 do
      tasks := ((a.(i), b.(j)), (i,j)) :: !tasks
    done done;
    !l
\end{ocaml}
Each task consists of row \of{a.(i)} and column \of{b.(j)} as first
tuple, together with position \of{(i,j)} as a second tuple.
The worker is receiving the first tuple as argument and computes
the dot product:
\begin{ocaml}
  let worker (ai, bj) =
    let c = ref 0 in
    for k = 0 to m-1 do c := !c + ai.(k) * bj.(k) done;
    !c
\end{ocaml}
The master is a one line function which receives the result \of{r}
from the worker and simply stores it into \of{c}, according to the
position contained in the input task. It produces no new task.
\begin{ocaml}
  let master (_, (i,j)) r = c.(i).(j) <- r; []
\end{ocaml}
Finally, the overall computation is started by invoking \of{compute}
as follows:
\begin{ocaml}
  let () = compute ~worker ~master !tasks
\end{ocaml}
The total number of tasks is $\of{n}\times\of{m}$.

Using the sequential implementation provided by \of{Functory} is as
simple as including the following line in the code:
\begin{ocaml}
  open Sequential
\end{ocaml}
Then the computation is roughly similar to the standard multiplication
described above. 

Now, let us assume we want to make use of a 4-core machine to perform
the same computation. 
This is achieved by just replacing the line above by
\begin{ocaml}
  open Cores
  let () = set_number_of_cores 4
\end{ocaml}
while the rest of the code remains unchanged.

Later, we may want to use a network of machines instead, for example
two machines called \texttt{orcus} and \texttt{belzebuth} with 4 and 8
cores respectively. Similar to the above, we turn the above lines into
\begin{ocaml}
  open Network
  let () = declare_workers ~n:4 "orcus"
  let () = declare_workers ~n:8 "belzebuth"
  open Same
\end{ocaml}
Here, \of{Same} is a module which is to be used when master and worker
are running the same executable. It provides a \of{compute} function
which has the same signature as in modules \of{Sequential} and
\of{Cores}, so that the rest of the code remains unchanged. Master and
worker processes are distinguished at run-time using an environment
variable \texttt{WORKER} which is set/unset.

If we need to write two different programs for master and worker, for
reason of binary incompatibility or any other reason, the library API
is providing functions to do that. If master and worker are still
compiled with the same version of \Ocaml, we use the \of{Poly} module
which provides a polymorphic API. Let us start with the worker program.
It now looks like:
\begin{ocaml}
  open Poly
  let worker (ai, bj) = ...
  let () = Worker.compute worker ()
\end{ocaml}
The \of{Worker.compute} function enters a loop which waits for
tasks sent by the master and returns results computed using \of{worker}.
The master program is almost the same as before. First, we replace
module \of{Same} with module \of{Poly}:
\begin{ocaml}
  open Network
  let () = declare_workers ~n:4 "orcus"
  let () = declare_workers ~n:8 "belzebuth"
  open Poly
\end{ocaml}
Tasks and \of{master} function are unchanged:
\begin{ocaml}
  let tasks = ...
  let master (_, (i,j)) r = ...
\end{ocaml}
Finally, we start the computation with \of{Master.compute}, which does
not have a \of{worker} parameter anymore:
\begin{ocaml}
  let () = Master.compute ~master tasks
\end{ocaml}

When master and worker programs are compiled with different
versions of the \Ocaml\ compiler, our library still provides
a monomorphic API over strings. As a consequence, we need to convert
tasks and results to and from strings in both master and worker.
The modified worker program then looks as follows:
\begin{ocaml}
  open Mono
  let worker (ai, bj) = ...
  let worker_string t = string_of_int (worker (task_of_string i))
  let () = Worker.compute worker_string ()
\end{ocaml}
The master program is modified in a similar way.
We simply replace \of{Poly} with \of{Mono} and encode/decode coefficients
as strings, as follows:
\begin{ocaml}
  let tasks = ... string_of_task ...
  let master (_, (i,j)) r = c.(i).(j) <- coeff_of_string r; []
\end{ocaml}
where \of{string_of_task} and \of{task_of_string} are user-defined
functions to convert tasks to and from strings.

\subsection{N-queens}\label{sec:n-queens}

The next example is the classical $N$-queens problem, where we
compute the total number of ways to place $N$ queens on a $N\times N$
chessboard in such a way no two queens attack each other.
We use a standard backtracking algorithm for this problem, which
places the queens one by one starting from the first row.
Distributing the computation is thus quite easy: we consider all
possible ways to place queens on the first $D$ rows and then perform
the subsequent search in parallel. Choosing $D=1$ will result in
exactly $N$ tasks; choosing $D=2$ will result in $N^2-3N+2$ tasks; and
so on.
% TODO: make a picture to illustrate positioning of 2 queens on the
% first 2 rows

Each task only consists of three integers and its result is one integer,
which is the total number of solutions for this task.
We make use of function \of{map_local_fold} from the derived API,
where \of{map} is performing the search and \of{fold} simply adds the
intermediate results.
In the network configuration, we make use of the \of{Network.Same}
module, workers and master being the same executable.

\subsection{Mandelbrot Set}

Drawing the Mandelbrot set is another classical example that could be
distributed easily, since the color of each point can be computed
independently of the others. 
Let us assume we are given the coordinates of the region to be drawn,
along with the width \of{w} and height \of{h} of the resulting image
(in pixels). 
Let us assume the total number of tasks $\of{t} \ge 1$ is given as a
parameter. It is immediate to split the image into \of{t} sub-images,
each of which is computed in parallel with and independently of the
others. For instance, the image could be split into horizontal slices
or, more generally, into rectangular blocks.

Each task is thus four floating-point numbers denoting the region
coordinates, together with two integers denoting the dimensions of the
sub-image to be drawn. The result of the task is a matrix of pixels,
of size $(\of{w}\times\of{h}) / \of{t}$. 
For instance, drawing a $800\times 600$ image using 20 tasks will
result in 20 sub-images of size $176,000$ bytes each,
assuming each pixel is encoded in four bytes.

In the network configuration, we deliberately choose to have two
different programs for master and workers, using the \of{Network.Poly}
module (actually, a single source code with a command line option).

% \subsection{Polynomial Multiplication}
% TODO?

\subsection{SMT Solvers}\label{sec:SMT}

Here we demonstrate the potential of our library for our application
needs as mentioned in the introduction. In this case study,
we consider 80 challenging verification conditions
(VC) obtained from the Why platform~\cite{filliatre07cav}.  Each
VC is stored in a file, which is accessible over
NFS. The purpose of the experiment is to check the validity of each VC
using several automated provers (namely Alt-Ergo, Simplify, Z3 and CVC3),
which are all installed on each of the machines in use.

The master program proceeds by reading the file names, turning them
into tasks by multiplying them by the number of provers (more than
300 tasks in total).
Each worker in turn invokes the given prover on the given file, within
a timeout limit.
Each task completes with one of the four possible outcomes: \emph{valid},
\emph{unknown} (depending on
whether the VC is valid or undecided by the prover), 
\emph{timeout} and \emph{failure}.

The result of each computation is a pair denoting the status and the
time spent in the prover call. The master collects these results and
sums up the timings for each prover and each possible status.

\section{Implementation Details}\label{sec:implem}

We now describe the implementations of the various modules introduced
in Section~\ref{sec:API}. The implementation of the \of{Sequential}
module is straightforward and does not require any explanation.

\subsection{Multiple Cores}

The \of{Cores} module implements the distributed computing library for
multiple cores on the same machine. 
% As illustrated in Section~\ref{sec:example}, 
It provides a function
\of{set_number_of_cores: int -> unit} to indicate the number of cores
to be used. The number passed to this function may be different from
the actual number of cores in the machine; it is rather the number of
tasks to be performed simultaneously.
% TODO comment more on the number of cores?
% Thus a program using the \of{Cores} module typically begins with 
% \begin{ocaml}
% open Cores
% let () = set_number_of_cores 8
% \end{ocaml}

The \of{Cores} module is implemented with \unix\ processes, using the
\of{fork} and \of{wait} system calls provided by the \of{Unix} library
of \Ocaml. The idea is pretty simple.  The \of{compute} function
maintains a global table of pending tasks and keeps track of the
number of idle cores.  Whenever there is a pending task and an idle
core, a new sub-process is created using \of{Unix.fork}; once
completed, the sub-process marshals the result into a 
local file. The
main process maintains a table mapping each sub-process ID to the
input task and the local file name. It waits for any completed
sub-process using \of{Unix.wait} and recovers the result from the
local file. Then function \of{master} is applied, which may generate
new tasks.  The main loop can be depicted in the following way:
\begin{flushleft}
  \quad  \textbf{while} pending tasks $\lor$ pending sub-processes \\
  \quad  \quad \textbf{while} pending tasks $\land$ idle cores \\
  \quad  \quad \quad create new sub-process for some task \\
  \quad  \quad \textbf{wait} for any completed sub-process \\
  \quad  \quad \quad push new tasks generated by \of{master} \\
\end{flushleft}
We also have an alternative implementation using
\unix\ pipes instead of local files.

The scheduling of tasks to the different cores is left to the
operating system, through \of{Unix.fork}. Thus it may be the case that
two tasks are executed on the same core, even if the declared number
of cores is less or equal than the actual number of cores on the machine.
% TODO: comment on user-provided scheduling?

\subsection{Network of Machines}

The \of{Network} module implements the distributed computing library
for a network of machines. 
% As illustrated in Section~\ref{sec:example}, 
It provides a function
\of{declare_workers: n:int -> string -> unit} to fill a table of
worker machines. 
% The argument \of{n} indicates the number of cores to
% be used in each worker machine, analogous to the \of{Cores}
% module. Thus a program using the \of{Network} module typically begins
% with
% \begin{ocaml}
% open Network
% let () = declare_workers ~n:12 "hostname1"
% let () = declare_workers ~n:8  "192.168.24.2"
% ...
% \end{ocaml}

The \of{Network} module is based on a traditional TCP-based client/server
architecture, where each worker is a server and the master is the
client of each worker. The main execution loop is similar to the one
in the \of{Cores} module, where distant processes on remote machines
correspond to sub-processes and idle cores are the idle cores 
of remote workers. In addition to this, it also involves issues of message
transfer and fault tolerance, which are subsequently described.

\subsubsection{Protocol}\label{sec:protocol}

Messages sent from master to workers could be any of the following kinds:
\begin{description}
\item[\of{Assign(id:int, f:string, x:string)}] This message assigns a
  new task to the worker, the task being identified by the unique
  integer \of{id}. The task to be performed is given by strings \of{f}
  and \of{x}, which are interpreted depending on the context.

\item[\of{Kill(id:int)}] This message tells the worker to kill the task
  identified by \of{id}.

\item[\of{Stop}] This message informs the worker about completion of
  the computation, so that it may choose to exit.

\item[\of{Ping}] This message is used to check if the worker is still
  alive, expecting a \of{Pong} message from the worker in return.
\end{description}
Messages sent by workers could be any of the following kinds:
\begin{description}
\item[\of{Pong}] This message is an acknowledgment for a \of{Ping}
  message from the master.

\item[\of{Completed(id:int, s:string)}] This message indicates the
  completion of a task identified by \of{id}, with result \of{s}.

\item[\of{Aborted(id:int)}] This message informs the master that the
  task identified by \of{id} is aborted, either as a response to a
  \of{Kill} message or because of a worker malfunction.
\end{description}
Our implementation of the protocol works across different
architectures, so that master and workers could be run on completely
different platforms w.r.t. endianness, version of \Ocaml\ and
operating system.

\subsubsection{Network Sub-modules} % TODO: change title

As mentioned in Section~\ref{sec:API}, the \of{Network} module
actually provides three different implementations, according to three
different execution scenarios. There are provided in three
sub-modules, as described below.

\paragraph{\of{Same}.} This module is used when master and workers are
running the same executable. The master and workers have to be
differentiated in some manner. We use an environment variable
\texttt{WORKER} for this purpose. When set, it indicates that the
executable acts as a worker. At run-time, a worker immediately 
enters a loop waiting for tasks from the master, without even getting
into the user code. 
As explained in Section~\ref{sec:API}, 
the master function has the following signature.
\begin{ocaml}
 val compute : 
      worker:('a -> 'b) -> 
      master:('a * 'c -> 'b -> ('a * 'c) list) -> ('a * 'c) list -> unit  
\end{ocaml}
The master uses marshaling to send both a closure of type 
\of{'a -> 'b} and a task of type \of{'a} to the worker. The resulting strings
are passed as argument \of{f} and \of{x} in message
\of{Assign}. Similarly, the worker uses marshaling to send back the
result of the computation of type \of{'b}, which is the argument
\of{s} in message \of{Completed}.

Though the ability to run the same executable helps a lot in deploying
the program in different machines, it comes at a small price. Since
the worker is not getting into the user code, closures which are
transmitted from the master cannot refer to global variables in the
user code. Indeed, the initialization code for these global variables
is never reached on the worker side. For instance, the Mandelbrot set
example could be written as follows:
\begin{ocaml}
  let max_iterations = 200
  let worker (xmi, xma, ymi, yma, w, h) =
    ... draw sub-image using max_iterations ...
\end{ocaml}
That is, the global function \of{worker} makes use of the global
variable \of{max_iterations}. The worker gets the function to compute
from the master, namely the closure corresponding to function
\of{worker} in that case, but on the worker side the initialization of
\of{max_iterations} is never executed.

One obvious solution is  not to use global variables in the
worker code. This is not always possible, though. 
To overcome this, the \of{Same} sub-module also provides a
\of{Worker.compute} function to start the worker loop manually from
the user code. This way, it can be started at any point, in particular
after the initialization of the required global variables.
Master and worker are still running the same executable, but are 
distinguished using a user-defined way (command-line argument,
environment variable, etc.).

\bigskip
There are situations where it is not possible
to run the same executable for master and workers.
For instance, architectures or operating systems could be different
across the network.
For that reason, the \of{Network} module provides two other
implementations. 

\paragraph{\of{Poly}.} When master and workers are compiled with the
same version of \Ocaml, we can no longer marshal closures but we can
still marshal polymorphic values. Indeed, an interesting property of
marshaling in \Ocaml\ is to be fully architecture-independent, as long
as a single version of \Ocaml\ is used. It is worth pointing out that
absence of marshaled closures
now enables the use of two different programs for master and workers. This
is not mandatory, though, since master and workers could still be
distinguished at run-time as in the previous case. 

On the worker side, the main loop is started manually using
\of{Worker.compute}. The computation to be performed on each task is
given as an argument to this function. It thus looks as follows:
\begin{ocaml}
  Worker.compute : ('a -> 'b) -> unit -> unit
\end{ocaml}
On the master side, the \of{compute} function is simpler than in the
previous case, as it has one argument less, and thus has the following
signature. 
\begin{ocaml}
  Master.compute : master:('a * 'c -> 'b -> ('a * 'c) list) -> ('a * 'c) list -> unit 
\end{ocaml}
For realistic applications, where master and workers are completely
different programs, possibly written by different teams, this is the
module of choice in our library, since it can still pass polymorphic
values over the network. The issues of marshaling are automatically
taken care of by \Ocaml\ run-time.

The derived API presented in Section~\ref{sec:derived} is adapted to
deal with the absence of closures. Exactly as the \of{compute}
function, each API now takes two forms, one for the master and another
for the workers. For example, \of{map_fold_ac} takes the following
forms.
\begin{ocaml}
  Worker.map_fold_ac : map:('a -> 'b) -> fold:('b -> 'b -> 'b) -> unit -> unit
  Master.map_fold_ac : 'b -> 'a list -> 'b
\end{ocaml}
It is the responsibility of the user to ensure type consistency between
master and workers.

\paragraph{\of{Mono}.} When master and workers are compiled using
different versions of \Ocaml, we can no longer use marshaling. 
As in the previous case, we split \of{compute} into two functions, one
for master and one for workers. In addition, values transmitted over
the network can only be strings. The signature thus takes the
following form.
\begin{ocaml}
  Worker.compute : (string -> string) -> unit -> unit
  Master.compute : master:(string * 'c -> string -> (string * 'c) list) -> 
                  (string * 'c) list -> unit
\end{ocaml}
Any other datatype for tasks should be encoded to/from strings. This
conversion is left to the user.
Note that the second component of each task is still polymorphic (of
type \of{'c} here), since it is local to the master.


% There is
% actually a single, generic implementation, which is then instantiated
% in three different ways, corresponding to the three scenarios.  The
% generic implementation assumes that only strings are being transmitted
% over the network, as evident from the \of{Assign} and \of{Completed}
% messages in Section~\ref{sec:protocol}. Depending on the
% implementation, these strings could be marshaled closures, marshaled
% values or simply user strings.

\subsubsection{Fault Tolerance}\label{sec:fault}

The main issue in any distributed computing environment is the ability
to handle faults, which is also a distinguishing feature of our
library.  Faults are mainly of two kinds: either a worker is stopped,
and possibly later restarted; or a worker is temporarily or
permanently unreachable on the network. To provide fault tolerance,
our master implementation is keeping track of the status of each
worker.  This status is controlled by two timeout parameters $T_1$ and
$T_2$ and \of{Ping} and \of{Pong} messages sent by master and workers,
respectively. There are four possible statuses for a worker:
\begin{description}
\item[\of{not connected}:] there is no ongoing TCP connection between
  the master and the worker;
\item[\of{alive}:] the worker has sent some message
  within $T_1$ seconds;
\item[\of{pinged}:] the worker has not sent any message within $T_1$
  seconds and the master has sent the worker a
  \of{Ping} message within $T_2$ seconds;
\item[\of{unreachable}:] the worker has not yet responded to the \of{Ping}
  message (for more than $T_2$ seconds).
\end{description}
% TODO picture
Whenever we receive a message from a worker, its status changes to
\of{alive} and its timeout value is reset.
\begin{center}
  \includegraphics{state.mps}
\end{center}

Fault tolerance is achieved by exploiting the status of workers as
follows. First, tasks are only assigned to workers with either
\of{alive} or \of{pinged} status. Second, whenever a worker executing
a task $t$ moves to status \of{not connected} or \of{unreachable}, the
task $t$ is rescheduled, which means it is put back in the set of
pending tasks. Whenever a task is completed, any rescheduled copy of
this task is either removed from the set of pending tasks or killed if
it was already assigned to another worker.

% The signature for the generic
% implementation is therefore as follows:
% \begin{ocaml}
%   generic_master:
%     assign_job:('a -> string * string) ->
%     master:('a * 'c -> string -> ('a * 'c) list)  ->
%     ('a * 'c) list -> unit
% \end{ocaml}
% The first argument \of{assign_job} is a function indicating the two
% strings that are passed in the \of{Assign} message of the protocol.
% We can now distinguish the three cases, based on \of{assign_job}:
% \begin{description}
% \item[\of{Same}:] in this case, \of{assign_job} simply returns
%   the marshaled closure together with the marshaled value.
% \item[\of{Poly}:] in this case, \of{assign_job} is simply
%   returning the marshaled value (the first string being meaningless in
%   this context).
% \item[\of{Mono}:] in this case, \of{assign_job} is immediately
%   returning its argument (the first string being meaningless in
%   this context).
% \end{description}

\section{Experiments}\label{sec:experiments}

Our library, \functory, is implemented in \Ocaml\ and is available
from \url{http://www.lri.fr/~filliatr/functory/}.
In this section, we show the experimental results pertaining to the
case studies of Section~\ref{sec:studies}.

\subsection{N-queens}

The following table shows execution times for various values of $N$
and our three different implementations: \of{Sequential}, \of{Cores}, and
\of{Network}. The purpose of this experiment is to measure the speedup
w.r.t. the sequential implementation. Therefore, all
computations are performed on the same machine, a 8 cores Intel Xeon
3.2 GHz running Debian Linux. The sequential implementation uses a
single core. The multi-core implementation uses the 8 cores of
the machine. The network implementation uses 8 workers running locally and
a master running on a remote machine (which incurs communication cost).

The first column shows the value of $N$.  The number of tasks is shown in
second column.  Then the last three columns show execution times in
seconds for the three implementations. The figures within brackets
show the speedup w.r.t. sequential implementation.
\begin{center}
  \begin{tabular}{|r|r|r|r|r|}
    \hline
    N & \#tasks  & \of{Sequential}& \of{Cores}                 & \of{Network} 
    \\\hline\hline
    16 &   16    &  15.2     &   2.04 (7.45$\times$) &  2.35  (6.47$\times$) 
    \\\hline
       &  210    &  15.2     &   2.01 (7.56$\times$) & 21.80  (0.69$\times$)
    \\\hline
    17 &   17    & 107.0     &  17.20 (6.22$\times$) & 16.20  (6.60$\times$)
    \\\hline
       &  240    & 107.0     &  14.00 (7.64$\times$) & 24.90  (4.30$\times$)
    \\\hline
    18 &   18    & 787.0     & 123.00 (6.40$\times$) & 125.00 (6.30$\times$)  
    \\\hline
       &  272    & 787.0     & 103.00 (7.64$\times$) & 124.00 (6.34$\times$)  
    \\\hline
    19 &   19    &6120.0     & 937.00 (6.53$\times$) & 940.00 (6.51$\times$)  
    \\\hline
       &  306    &6130.0     & 796.00 (7.70$\times$) & 819.00 (7.48$\times$)
    \\\hline
  \end{tabular}
\end{center}
From the table above, it is clear that the \of{Cores} and \of{Network}
implementations provide a significant speedup. As evident from the
last row, the speedup is almost 8, which is also the number of
cores we use.  It is also evident from the last column that the
\of{Network} implementation performs significantly better when the
computation time dominates in the total execution time.  The two extreme
cases correspond to the second and the last row: in the second row, the
communication time dominates and is in fact more than 91\%\ of the
total execution time; on the other hand, for the last row
communication time amounts to just 4.6\%\ of the total execution time.
As expected, the network implementation is only beneficial when the
computation time for each individual task is significant, which is the
case in realistic examples.

\subsection{Mandelbrot Set}

This benchmark consists in drawing the fragment of the Mandelbrot set
with lower left corner $(-1.1, 0.2)$ and upper right corner $(-0.8,
0.4)$, as a $9,000\times6,000$ image. The sequential computation of
this image consumes 29.4 seconds. For \of{Cores} and \of{Network}
implementations, the computation times in seconds are tabulated below.

% TODO compute ratios for Network
\begin{center}
  \begin{tabular}{|r|r|r|r|r|}
    \hline
    \#cores  &\#tasks & \of{Cores} & \of{Network} \\
    \hline\hline
    2       & 10 & 15.8       (1.86) &  20.3       \\
            & 30 & 15.7       (\underline{1.87}) &  18.7       \\
            & 100 & 16.1      (1.83) &  19.8       \\
            & 1000 & 19.6     (1.50) &  38.6      \\
    \hline
    4       & 10 & 9.50       (3.09)  &  14.4       \\
            & 30 & 8.26       (\underline{3.56})  &  11.4   \\
            & 100 & 8.37      (3.51)  &  11.4   \\
            & 1000 & 10.6     (2.77)  &  20.5    \\
    \hline
    8       & 10 & 9.40       (3.13)  &  12.6      \\
            & 30 & 4.24       (\underline{6.93})  &   7.6      \\
            & 100 & 4.38      (6.71)  &   7.5      \\
            & 1000 & 6.86     (4.29)  &  11.3      \\
    \hline
  \end{tabular}
\end{center}
The best timings are achieved for the \of{Cores} configuration, where
communications happen within the same machine and are thus cheaper.
There are two significant differences with respect to the n-queens
benchmark.  On one hand, the number of tasks can be controlled more
easily than in the case of n-queens. We experimentally figured out the
optimal number of tasks to be 30. One the other hand, each computation
result is an image, rather than just an integer as in the case of
n-queens. Consequently, communication costs are much greater. 
In this particular experiment, the total size of the results
transmitted is more than 200 Mb.

\subsection{Matrix Multiplication}

This benchmark was inspired by the PASCO'10 programming contest~\cite{PASCO}.
It consists of multiplication of
two square matrices of dimension 100, that is $\of{n} = \of{p} =
\of{m} = 100$, with integer coefficients.
Coefficients have several thousands of digits, hence we use
GMP~\cite{GMP} to handle operations over coefficients.

We compare the performances of two different implementations. In the
first one, called \of{mm1}, each task consists of the computation of a single
coefficient of the resultant matrix, as described in Section~\ref{sec:matrix}.
In the second one, called \of{mm2}, each task consists of the
computation of a whole row of the resultant matrix.
As a consequence, the total number of tasks in
$\of{n}\times\of{m}=10,000$ for \of{mm1} and only $\of{n}=100$ for \of{mm2}.
The experimental results (in seconds) are tabulated below.
\begin{center}
  \begin{tabular}{|r|r|r|}
    \hline
    & \of{mm1}       & \of{mm2}  \\
    & (10,000 tasks) & (100 tasks) \\
    \hline\hline
\of{Sequential} & 20.3 &  20.2 \\
\hline
 \of{Cores} 
 (2 cores)     &   22.7  &  11.3\\
 (4 cores)     &   12.3  &   6.1\\
 (6 cores)     &    8.6  &   4.3\\
 (8 cores)     &    8.0  &   3.5\\
 \hline
  \end{tabular}
\end{center}
We do not include results for the network configuration, as they do
not achieve any benefit with respect to the sequential
implementation. The reason is that the communication cost dominates the
computation cost in such a way that the total execution time is
always greater than 30 seconds. Indeed, irrespective of the
implementation (\of{mm1} or \of{mm2}), the total size of the
transmitted data is 
$O(\of{n}\times\of{m}\times\of{p})$, which in our case amounts to
billions of bytes.

A less naive implementation would have the worker read the input matrices
only once, \emph{e.g.} from a file, and then have the master send only
row and column indices. This would reduce the communication cost to
$O(\of{n}\times\of{m})$ only.

%             mm1     mm2

% # tasks    10,000   100

% sequential  20.3   20.2

% cores 2     22.7   11.3
% cores 4     12.3    6.1
% cores 6      8.6    4.3
% cores 8      8.0    3.5

% network = workers on moloch + master on belzebuth

% cores 2     93     32.3
% cores 4     87     33.7
% cores 6     92     33.5
% cores 8     88     32.5




\subsection{SMT Solvers}

As explained in Section~\ref{sec:SMT}, this benchmark consists of 80
verification conditions, each being checked by 4 different SMT
solvers. Each task is executed with a timeout limit of 1 minute.
Our computing
infrastructure for this experiment consists of 3 machines with 4, 8 and 8 cores
respectively. 
The figure below shows the total time in minutes spent by each prover
for each possible outcome.
\begin{center}
  \begin{tabular}{|r||r|r|r|r|}
    \hline
    prover   & valid & unknown & timeout & failure
    \\\hline\hline
    Alt-ergo & 406.0 & 3.0   &  11400.0 & 0.0       
    \\\hline
    Simplify &  0.5   & 0.4   &  1200.0 & 222.0   
    \\\hline
    Z3       & 80.7   & 0.0   &  1800.0 & 1695.0   
    \\\hline
    CVC3     & 303.0  & 82.7  &  4200.0 & 659.0   
    \\\hline
  \end{tabular}
\end{center}
These figures sum up to more than 6 hours if provers were executed
sequentially. However, using our library and our 3-machine
infrastructure, it completes in 22 minutes and 37 seconds, giving us a
speedup of more than 16$\times$. We are still far away from the ideal
ratio of 20$\times$ (we are using 20 cores), since some provers are
allocating a lot of memory and time spent in system calls is not
accounted for in the total observed time. However, a ratio of
16$\times$ is already a significant improvement for our day-to-day
experiments. 

\section{Conclusion and Future Work}\label{sec:future}

We presented a distributed programming environment for functional
programming. The main features are the genericity of the interface,
which makes use of polymorphic higher-order functions, and the ability
to easily switch between sequential, multi-core, and network
implementations. In particular, \functory\ allows to use the same
executable for master and workers, which makes the deployment of small programs
immediate --- master and workers being only distinguished by an
environment variable. \functory\ also allows master and workers to be
completely different programs, which is ideal for large scale deployment.
Another distinguishing feature of our library is a
robust fault-tolerance mechanism which relieves
the user of cumbersome implementation details.
Finally, \functory\ also allows to cascade several distributed
computations inside the same program.

\paragraph{Related Work.}
Closest to the approach in this paper is Yohann Padioleau's MapReduce
implementation in \Ocaml~\cite{poor-man-mapreduce}.  It is built on
top of OCamlMPI~\cite{ocamlMPI}, while our approach uses a homemade
protocol for message passing.  Currently, we have less flexibility
w.r.t. deployment of the user program than OCamlMPI; on the other
hand, we provide a 
more generic API together with fault tolerance.  There are other
distributed computing libraries on top of which one could implement
the library discussed in this paper. \JoCaml~\cite{jocaml} is one of
them. However, \JoCaml\ does not provide fault tolerance, which is
indispensable in a distributed setting. The user has to include code
for fault tolerance, as already demonstrated in some \JoCaml\
experiments~\cite{mandel2008}. 

There are other implementations of distributed computing in the
context of functional programming. One is the Disco
project~\cite{disco}, which implements exactly Google's MapReduce in
Erlang~\cite{erlang}. Our library, on the contrary, is not an \Ocaml\
implementation of Google's MapReduce.
There are other ways to exploit multi-core architectures. One of these
is data parallelism, which is also relevant in the functional
programming setting~\cite{parallel-haskell}. Our work
does not target data parallelism at all.

\paragraph{Future Work.}
There are still some interesting features that could be added to our
library. 
\begin{itemize}
\item 
  One is the ability to efficiently assign tasks to workers depending
  on resource parameters, such as data locality, CPU power, memory,
  etc. This could be achieved by providing the user with the means to control
  task scheduling.  
\item 
  Another interesting feature could be the ability
  to add or remove machines dynamically. Currently, our library assumes
  that the list of machines to be used is given \emph{a priori}, as
  part of the code. An alternative would be to read machine names from
  a file, watched periodically by the master.
\item
  Our library provides limited support for displaying real-time
  information about computations and communications. Processing and storing
  information about workers and tasks locally in the master is straightforward; 
  monitoring it in real-time could be done using
  \textsf{Ocamlviz}~\cite{ocamlviz}. 
\item 
  One very nice feature of Google's MapReduce is the possibility to
  use redundantly several idle workers on the same tasks
  for speedup when reaching the end of computation.
  Since we already have the fault tolerance implemented, this
  optimization should be straightforward to add to our library.
\end{itemize}
We intend to enrich our library with all above features.

%\appendix


\paragraph{Acknowledgements.}
The authors are grateful to the ProVal team for support and comments
on early versions of the library and of this paper.

%\vfill\pagebreak
\bibliographystyle{plain}
\bibliography{./biblio}

\end{document}

% LocalWords:  parallelize functor parameterized indices endianness monomorphic
% LocalWords:  genericity executables OCamlMPI MapReduce généricité marshalling

%%% Local Variables: 
%%% mode: latex
%%% ispell-local-dictionary: "francais"
%%% TeX-master: t
%%% End: 

% LocalWords:  sérialisation boutisme inatteignables observationnellement
% LocalWords:  sous-modules
