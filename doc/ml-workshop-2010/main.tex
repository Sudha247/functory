%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[preprint]{sigplanconf}

% The following \documentclass options may be useful:
%
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath,color,url}

\newcommand{\Ocaml}{OCaml}
\newcommand{\functory}{\textsf{Functory}}
\newcommand{\JoCaml}{Jo{\&\!}Caml}
\newcommand{\unix}{\textsc{Unix}}

\begin{document}

\conferenceinfo{Workshop on ML 2010}{September 26, 2010, Baltimore, Maryland.} 
\copyrightyear{2010} 
\copyrightdata{[to be supplied]} 

\titlebanner{submitted to Workshop on ML 2010}        % These are ignored unless
\preprintfooter{Distributed Computing for the Common Man}   % 'preprint' option specified.

\title{Distributed Computing for the Common Man}
%\subtitle{Subtitle Text, if any}

\authorinfo{Jean-Christophe Filli\^{a}tre}
           {CNRS, LRI, Univ Paris-Sud 11, Orsay F-91405\\
            INRIA Saclay - \^{I}le-de-France, ProVal, Orsay, F-91893}
           {filliatr@lri.fr}
\authorinfo{K. Kalyanasundaram}
           {ProVal, INRIA Saclay - \^{I}le-de-France, Orsay, F-91893}
           {kalyan.krishnamani@inria.fr}

\maketitle

\begin{abstract}
  We present a distributed computing library for
  functional programming languages. The main features of this library
  include (1) a generic API, (2) several implementations to
  adapt to different computing environments such as sequential,
  multi-core or network, and (3) a reliable fault-tolerance mechanism.
  This paper describes the motivation behind this work, as well as
  the design and implementation of the library. It also demonstrates
  the potential of the library using realistic experiments.
\end{abstract}

% \category{D1.1}{Programming techniques}{Applicative (Functional) Programming}

% \terms
% Distributed Computing, Functional Programming

%\keywords

\section{Introduction}

This paper introduces a generic library for distributed computing. This
work was initially motivated by the computing needs that exist in our
own research team. Our applications include large-scale deductive
program verification, which amounts to checking the validity of a
large number of logical formulas using a variety of automated theorem
provers~\cite{filliatre07cav}. Our computing infrastructure consists
of a few powerful multi-core machines (typically 8 to 16 cores) and
several desktop PCs (typically dual-core). However, for our
application needs, there is no library that helps in exploiting such a
computing infrastructure in our favorite functional programming
language.  Hence we designed and implemented such a
library\footnote{This library is implemented in \Ocaml, but the
  implementation should be straightforward in any functional
  programming language.}, which is the subject of this paper.

The distributed computing library presented in this paper is not a
library that helps in parallelizing computations. Rather, it provides
facilities for reliable, distributed execution of parallelizable
computations. In particular, it provides a set of APIs that allows the
execution of large-scale parallelizable computations, very relevant to
our application needs (and presumably relevant to a variety of
real-world applications), over multiple cores in the same machine or
over a network of machines. 
The most important features of our library are the following:
\begin{itemize}
\item \emph{Genericity}: 
  it allows various patterns of polymorphic computations;
\item \emph{Simplicity}: switching between multiple cores on the same
  machine and a network of machines is as simple as changing a couple
  of lines of code;
\item \emph{Task distribution and fault-tolerance}: 
  it relieves the user from implementing task distribution routines
  and reliable fault-tolerance mechanisms.
\end{itemize}
It is worth noting that the library is not targeted at applications
running on server farms, crunching enormous amounts of data. Neither
is it limited to research labs possessing few powerful
machines. It serves a variety of users and a wide
spectrum of needs, from desktop PCs to networks of machines, and hence
the title.




% In addition to the above, distributed computing environments also
% implement mechanisms for fault tolerance, efficient storage, and
% distribution of tasks. This is required to handle network failures
% that may occur, as well as to optimize the usage of machines in the
% network. Another concern of importance is the transmission of messages
% over the network. This requires efficient 
% \emph{marshaling} of data, that is encoding and decoding of data 
% for transmission over different computing environments.  It is desirable to
% maintain architecture independence while transmitting marshalled data,
% as machines in a distributed computing environment often run on
% different hardware architectures and make use of varying software
% platforms. For example, machine word size or endianness may be different
% across machines on the network.

% \paragraph{A Functional Programming Approach.}
% Our work was initially inspired by Google's
% MapReduce\footnote{Ironically, Google's approach itself was inspired
%   by functional programming primitives.}. However, our functional
% programming environment allows us to be more generic. 
% The main idea behind our approach is that
% workers may implement any polymorphic function:
% \begin{ocaml}
%   worker: 'a -> 'b
% \end{ocaml}
% where \of{'a} denotes the type of tasks and \of{'b} the type of results.
% Then the master is given a list of initial tasks, together with a
% function to handle the results:
% \begin{ocaml}
%   master: ('a -> 'b -> 'a list) -> 'a list -> unit
% \end{ocaml}
% The function passed to the master is applied whenever a result is
% available and may in turn generate new tasks (hence the return type
% \of{'a list}).  The master is executed as long as there are pending
% tasks.

% Our library makes use of \Ocaml's marshaling capabilities as much as
% possible. Whenever master and worker executables are exactly the same,
% we can marshal polymorphic values and closures. However, it is not
% always possible to have master and workers running the same
% executable. In this case, we cannot marshal closures anymore but we
% can still marshal polymorphic values as long as the same version of
% \Ocaml\ is used to compile master and workers. When different versions
% of \Ocaml\ are used, we can no longer marshal values but we can still
% transmit strings between master and workers. Our library adapts to all
% these situations, by providing several APIs.

% \medskip
% The paper is organized as follows. In Section~\ref{sec:API}, we
% describe the main idea behind the generic API, and introduce
% \of{master} and \of{worker} programs. Section~\ref{sec:example}
% illustrates the API usage with an example.
% In Section~\ref{sec:derived}, we
% derive some interesting functions from the generic
% API. Section~\ref{sec:implem} delves into the implementation details
% of our library, while Section~\ref{sec:experiments} illustrates the
% potential of the presented library through experimental evaluation. We
% compare our approach with relevant related work in
% Section~\ref{sec:future} and outline our future work.

Our work was initially inspired by Google's
MapReduce\footnote{Ironically, Google's approach itself was inspired
  by functional programming primitives.}, from where we borrow some 
terminology:
\begin{itemize}
\item A notion of \emph{tasks} which denote atomic computations
  to be performed in a distributed manner; 
\item A set of processes (possibly executing on remote machines)
  called \emph{workers} that perform
  the tasks, producing results;
\item A single process called a \emph{master} which is in charge
  of distributing the tasks among the workers and managing results
  produced by the workers.
\end{itemize}

Our library, \functory, is implemented in \Ocaml.
It makes use of \Ocaml's marshaling capabilities as much as
possible. Whenever master and worker executables are exactly the same,
we can marshal polymorphic values and closures. However, it is not
always possible to have master and workers running the same
executable. In this case, we cannot marshal closures anymore but we
can still marshal polymorphic values as long as the same version of
\Ocaml\ is used to compile master and workers. When different versions
of \Ocaml\ are used, we can no longer marshal values but we can still
transmit strings between master and workers. Our library adapts to all
these situations, by providing several APIs.


Our library is available
at \url{http://www.lri.fr/~filliatr/functory/}.
A report is also available there, which provides more
technical details. 

\section{API}\label{sec:API}

The main function of our API combines these three notions of tasks,
master and workers into a single higher-order polymorphic function
\of{compute}:
\begin{ocaml}
  val compute : 
    worker:('a -> 'b) -> 
    master:('a * 'c -> 'b -> ('a * 'c) list) -> 
    ('a * 'c) list -> unit
\end{ocaml}
Tasks are pairs, of type \of{'a * 'c}, where the first component is
passed to the worker and the second component is local to the master.
The \ocaml{worker} function should be pure and is executed in parallel
in all worker processes. The function \ocaml{master}, on the
contrary, can be impure and is only executed in the master process.
The \ocaml{master} function typically stores results 
in some internal data structure.

Our library provides not just a single \ocaml{compute}
function as above, but instead five different versions
depending on the execution context:
\begin{enumerate}
\item \textbf{Purely sequential execution:}
  this is mostly intended for debugging;

\item \textbf{Several cores on the same machine:} 
  this implementation makes use of \unix\ processes and 
  provides a polymorphic \ocaml{compute} function;

\item \textbf{Same executable run on master and worker machines:}
  this implementation makes use of the ability to marshal \Ocaml\
  closures and  provides a polymorphic \ocaml{compute}
  function.
  Depending on whether the program is run as a master or as a worker,
  the relevant arguments of \ocaml{compute} are used.

\item \textbf{Master and workers are different programs, compiled with
    the same version of \Ocaml:} 
  we can no longer marshal closures but we can still
  marshal polymorphic values. As a consequence, 
  the \ocaml{compute} function is split into two
  polymorphic functions, to implement the master and workers 
  respectively:%
\vspace{-1em}
\begin{ocaml}
val Worker.compute : ('a -> 'b) -> unit -> unit
val Master.compute : 
  ('a * 'c -> 'b -> ('a * 'c) list) -> 
  ('a * 'c) list -> unit
\end{ocaml}
\item \textbf{Master and workers are different programs, not even
    compiled with the same version of \Ocaml:} we can no
  longer use marshaling, so the
  \ocaml{compute} function is split into two monomorphic functions
  over strings:%
\vspace{-1em}
\begin{ocaml}
val Worker.compute : (string -> string) -> unit -> unit
val Master.compute : 
  (string * 'c -> string -> (string * 'c) list) -> 
  (string * 'c) list -> unit
\end{ocaml}
  Note that the second component of each task is still polymorphic (of
  type \ocaml{'c} here), since it is local to the master.
\end{enumerate}

% Our library is organized into three modules: \of{Sequential} for the
% pure sequential implementation, \of{Cores} for multiple cores on the
% same machine and \of{Network} for a network of machines, respectively.
% The \of{Network} module itself is organized into three sub-modules,
% called \of{Same}, \of{Poly} and \of{Mono},
% corresponding to contexts 3, 4 and 5 above. 
% The next section demonstrates the use of these modules.

In addition to this low-level API, our library also provides more
traditional \of{map} and \of{fold} higher-order functions with
distributed computation. For instance, 
\begin{ocaml}
  val map_local_fold : 
    map:('a -> 'b) -> fold:('c -> 'b -> 'c) -> 
    'c -> 'a list -> 'c 
\end{ocaml}
applies \of{map} to each element of the list in a distributed way and
folds the results locally to the master. 

\section{Illustrative Example}\label{sec:example}

Let us assume we need to compute a sum such as
\begin{displaymath}
  s(n) = \sum_{i=0}^{n}f(i)
\end{displaymath}
where $f$ is a pure and computationally expensive function.
Using the sequential implementation and the low-level \of{compute}
function, the code looks like:
\begin{ocaml}
1.  open Sequential
2.  let f i = ...
3.  let n = int_of_string Sys.argv.(1)
4.  let tasks = ... (* the lists [0; 1; ...; n] *)
5.  let s = ref 0
6.  let master (i,()) fi = s := !s + fi; []
7.  let () = compute ~worker:f ~master tasks
\end{ocaml}
To switch to the multi-core implementation, say with 4 cores, we
simply replace line 1 with
\begin{ocaml}
1a.  open Cores
1b.  let () = set_number_of_cores 4
\end{ocaml}
Similarly, in order to use a network of two machines \texttt{m1} and
\texttt{m2}, with 4 and 8 cores respectively, line 1 is replaced with
\begin{ocaml}
1a.  open Network
1b.  let () = declare_workers ~n:4 "m1"
1c.  let () = declare_workers ~n:8 "m2"
\end{ocaml}

We could simplify the code even further using the derived API, for
example by replacing lines 5-7 with
\begin{ocaml}
5.  let s = map_local_fold ~map:f ~fold:(+) 0 tasks
\end{ocaml}


\section{Experiments}\label{sec:experiments}

Here we demonstrate the potential of our library for using the
classical $N$-queens problem and large-scale verification using SMT-solvers.

\paragraph{N-queens.} 
The following table shows execution times\footnote{machine
  characteristics: 8 cores Intel Xeon 3.2 GHz running Debian Linux.} 
in seconds for various values of $N$
and our three different implementations (sequential, 8 cores on a
single machine and a network of 8 workers).
\begin{center}
  \begin{tabular}{|r|r|r|r|r|}
    \hline
    N & \#tasks  & sequential& cores                 & network 
    \\\hline\hline
    16 &   16    &  15.2     &   2.04 (7.45$\times$) &  2.35  (6.47$\times$) 
    \\\hline
       &  210    &  15.2     &   2.01 (7.56$\times$) & 21.80  (0.69$\times$)
    \\\hline
    17 &   17    & 107.0     &  17.20 (6.22$\times$) & 16.20  (6.60$\times$)
    \\\hline
       &  240    & 107.0     &  14.00 (7.64$\times$) & 24.90  (4.30$\times$)
    \\\hline
    18 &   18    & 787.0     & 123.00 (6.40$\times$) & 125.00 (6.30$\times$)  
    \\\hline
       &  272    & 787.0     & 103.00 (7.64$\times$) & 124.00 (6.34$\times$)  
    \\\hline
    19 &   19    &6120.0     & 937.00 (6.53$\times$) & 940.00 (6.51$\times$)  
    \\\hline
       &  306    &6130.0     & 796.00 (7.70$\times$) & 819.00 (7.48$\times$)
    \\\hline
  \end{tabular}
\end{center}
% From the table above, it is clear that the \of{Cores} and \of{Network}
% implementations provide a significant speedup. As evident from the
% last row, the speedup is almost 8, which is also the number of
% cores we use.  It is also evident from the last column that the
% \of{Network} implementation performs significantly better when the
% computation time dominates in the total execution time.  The two extreme
% cases correspond to the second and the last row: in the second row, the
% communication time dominates and is in fact more than 91\%\ of the
% total execution time; on the other hand, for the last row
% communication time amounts to just 4.6\%\ of the total execution time.
% As expected, the network implementation is only beneficial when the
% computation time for each individual task is significant, which is the
% case in realistic examples.

\paragraph{SMT Solvers.}
Our setup consists of 3 machines with 4, 8 and 8 cores
respectively and the benchmark consists of
80 challenging verification conditions (VC).  
The purpose of the experiment is to check the validity of each VC
using the four SMT-solvers Alt-Ergo, Simplify, Z3 and CVC3.
With a timeout of 1 minute, each task completes 
with one of the four possible outcomes: \emph{valid},
\emph{unknown} (depending on
whether the VC is valid or undecided by the prover), 
\emph{timeout} and \emph{failure}.
Timings in minutes are the following:
\begin{center}
  \begin{tabular}{|r||r|r|r|r|}
    \hline
    prover   & valid & unknown & timeout & failure
    \\\hline\hline
    Alt-ergo & 406.0 & 3.0   &  11400.0 & 0.0       
    \\\hline
    Simplify &  0.5   & 0.4   &  1200.0 & 222.0   
    \\\hline
    Z3       & 80.7   & 0.0   &  1800.0 & 1695.0   
    \\\hline
    CVC3     & 303.0  & 82.7  &  4200.0 & 659.0   
    \\\hline
  \end{tabular}
\end{center}
These figures sum up to more than 6 hours if provers were executed
sequentially. However, using our library and our 3-machine
infrastructure, it completes in 22 minutes and 37 seconds, giving us a
speedup of more than 16$\times$. 

\end{document}

% LocalWords:  parallelize functor parameterized indices endianness monomorphic
% LocalWords:  genericity executables OCamlMPI MapReduce
